# Deepseek R1 Hyperparameter Tuner ğŸ›ï¸

An intuitive, modular CLI tool for tuning Deepseek R1 hyperparameters. Think of it as your friendly control panel for fine-tuning your model - no more wrestling with complex command-line arguments!

## âœ¨ Features

- ğŸ¨ Beautiful, colored interface with interactive menus
- âœ… Real-time parameter validation
- ğŸ’¾ Automatic configuration saving and loading
- ğŸ“Š Visual parameter organization
- ğŸ”„ Smart defaults for quick starts
- ğŸ“ Comprehensive logging system
- ğŸ”§ Easy reset to default settings
- ğŸ“¦ Modular, maintainable codebase

## ğŸ—ï¸ Project Structure

```
deepseek_tuner/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ main.py              # Application entry point
â”œâ”€â”€ config/             # Configuration management
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ manager.py      # Handles loading/saving configs
â”‚   â””â”€â”€ defaults.py     # Default parameter values
â”œâ”€â”€ ui/                 # User interface components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ menu.py        # Interactive menu system
â”‚   â”œâ”€â”€ display.py     # Output formatting
â”‚   â””â”€â”€ validators.py  # Input validation
â”œâ”€â”€ utils/             # Utility functions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger.py      # Logging system
â””â”€â”€ data/              # Data storage
    â”œâ”€â”€ logs/          # Log files
    â””â”€â”€ backups/       # Configuration backups
```

## ğŸš€ Quick Start

1. Clone this repository:
   ```bash
   git clone https://github.com/yourusername/deepseek-tuner
   cd deepseek-tuner
   ```

2. Create a virtual environment (recommended):
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

4. Run the tuner:
   ```bash
   python -m deepseek_tuner.main
   ```

## ğŸ® Usage Guide

The tuner organizes parameters into three main categories:

### Model Parameters
- `batch_size`: Number of samples processed in one forward/backward pass
- `learning_rate`: Step size for gradient descent optimization
- `warmup_steps`: Number of steps for learning rate warmup
- `weight_decay`: L2 regularization factor
- `gradient_accumulation_steps`: Number of steps to accumulate gradients
- `max_grad_norm`: Maximum gradient norm for gradient clipping
- `num_epochs`: Number of training epochs

### Training Parameters
- `fp16`: Enable mixed precision training
- `distributed`: Enable distributed training
- `seed`: Random seed for reproducibility
- `logging_steps`: Frequency of logging
- `save_steps`: Frequency of model checkpointing
- `eval_steps`: Frequency of evaluation

### Data Parameters
- `max_seq_length`: Maximum sequence length for input text
- `train_file`: Path to training data file
- `validation_file`: Path to validation data file
- `preprocessing_num_workers`: Number of workers for data preprocessing

## ğŸ”§ Configuration Management

The tool provides several ways to manage your configurations:

### Saving Configurations
- Configurations are automatically saved to `data/config.yaml`
- Backups are created automatically before major changes
- Manual saves available through the menu

### Resetting to Defaults
- Reset to default values at any time through the menu
- Automatic backup creation before reset
- Confirmation required to prevent accidents

### Logging System
- Daily log files in `data/logs/`
- Tracks all parameter changes and actions
- View recent logs through the menu
- Helps diagnose any issues

## ğŸƒâ€â™‚ï¸ Development Guide

To contribute to the project:

1. Fork the repository
2. Create a feature branch
3. Write clear, documented code
4. Add tests if necessary
5. Submit a pull request

### Testing
Run tests with:
```bash
python -m pytest tests/
```

### Code Style
We follow PEP 8 guidelines. Format your code with:
```bash
black .
```

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ™‹â€â™‚ï¸ Support

If you encounter any issues or have questions:

1. Check the logs in `data/logs/`
2. Check existing issues on GitHub
3. Create a new issue if needed
4. Join our community discussions